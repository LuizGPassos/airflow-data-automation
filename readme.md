# Projeto de Automa√ß√£o de Dados com Apache Airflow

Este projeto foi desenvolvido como parte do curso de **Apache Airflow** da **Alura**, com o objetivo de aplicar conceitos de orquestra√ß√£o de dados, automa√ß√£o de tarefas e cria√ß√£o de pipelines ETL utilizando o Airflow.

## Finalidade do Projeto

A proposta central do projeto √©:

- **Automatizar a coleta de dados meteorol√≥gicos** de uma API externa.
- **Executar essa coleta de forma recorrente (semanalmente ou diariamente)**, usando DAGs do Airflow.
- **Realizar um pequeno processo de ETL**, onde os dados s√£o extra√≠dos da API, organizados em estruturas tabulares e salvos como arquivos CSV.
- Demonstrar o uso de operadores b√°sicos do Airflow: `EmptyOperator`, `BashOperator` e `PythonOperator`.

Essa automa√ß√£o √© ideal para casos em que √© necess√°rio coletar dados de maneira constante e sistem√°tica, como para alimentar dashboards, relat√≥rios ou an√°lises.

---

## Ferramentas e Tecnologias

- **[Apache Airflow](https://airflow.apache.org/)** ‚Äì Orquestra√ß√£o dos fluxos de trabalho.
- **Python 3.9** ‚Äì Linguagem base do projeto.
- **`pandas`** ‚Äì Manipula√ß√£o e salvamento de dados tabulares.
- **`python-dotenv`** ‚Äì Carregamento de vari√°veis de ambiente (.env).
- **Sistema Operacional**: Ubuntu

---

## Estrutura dos DAGs

A pasta `dags/` cont√©m os DAGs desenvolvidos:

### üîπ `first_dag.py`

Um DAG simples criado com fins did√°ticos, contendo:

- **3 tarefas fict√≠cias (EmptyOperator)** para ilustrar depend√™ncias e execu√ß√£o paralela/sequencial.
- **1 tarefa com BashOperator** que cria um diret√≥rio com timestamp baseado no `data_interval_end`.

**Fluxo da DAG:**
```text
task_1
  ‚îú‚îÄ‚îÄ> task_2
  ‚îî‚îÄ‚îÄ> task_3 ‚îÄ‚îÄ> make_dir (cria√ß√£o de diret√≥rio local com data)
```

Esse DAG exemplifica a estrutura e execu√ß√£o de tarefas no Airflow, sem l√≥gica de neg√≥cio envolvida.

---

### üîπ `weather_data.py`

Este √© o DAG principal e funcional do projeto.

**Objetivo:** automatizar a coleta de dados meteorol√≥gicos da API [Open-Meteo](https://open-meteo.com/), salvando os dados de clima em um CSV.

#### Componentes da DAG:

- `extract_weather_data()`:  
  Fun√ß√£o Python que consome a API, coleta dados de temperatura di√°ria m√≠nima e m√°xima, e estrutura os dados em um DataFrame com `pandas`.

- `save_to_csv()`:  
  Fun√ß√£o que recebe os dados da tarefa anterior, converte em CSV e salva em disco no diret√≥rio `data/`.

#### L√≥gica do Fluxo:

```text
start (EmptyOperator)
   ‚Üì
extract_weather_data (PythonOperator)
   ‚Üì
save_to_csv (PythonOperator)
   ‚Üì
end (EmptyOperator)
```

O DAG roda semanalmente (`@weekly`) e salva arquivos CSV com a previs√£o da semana, automatizando completamente a coleta e persist√™ncia local.

---

## Uso de Vari√°veis de Ambiente

A DAG `weather_data.py` utiliza vari√°veis de ambiente (por exemplo, `API_KEY`) para proteger dados sens√≠veis. Essas vari√°veis s√£o carregadas via `.env` e lidas no script com a biblioteca `python-dotenv`.

```python
load_dotenv()
api_key = os.getenv("API_KEY")
```

> O arquivo `.env` **n√£o est√° versionado** por motivos de seguran√ßa.

---

## O que foi aprendido no projeto

- Conceitos de DAGs, operadores e depend√™ncias no Apache Airflow.
- Como estruturar pipelines ETL com Python e Airflow.
- Leitura de vari√°veis externas com `.env`.
- Modulariza√ß√£o de tarefas com `PythonOperator` e `BashOperator`.
- Persist√™ncia de dados em arquivos CSV.
- Orquestra√ß√£o de tarefas em ambientes de produ√ß√£o de dados.

---

**Desenvolvido como parte do curso [Apache Airflow da Alura](https://cursos.alura.com.br/course/apache-airflow-primeiro-pipeline-dados).**
